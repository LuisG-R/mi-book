{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5334554e",
   "metadata": {},
   "source": [
    "# Featurization notebook\n",
    "\n",
    "*Authors: Enze Chen and Mark Asta (University of California, Berkeley)*\n",
    "\n",
    "```{note}\n",
    "This is an interactive exercise, so you will want to click the {fa}`rocket` and open the notebook in DataHub (or Colab for non-UCB students).\n",
    "```\n",
    "\n",
    "## Learning objectives\n",
    "\n",
    "This notebook discuss featurization and how we can apply our domain knowledge to create **physically-meaningful features** for ML applications.\n",
    "By the end of this notebook, you will be able to:\n",
    "1. Articulate why it's important to use physically-meaningful features for MI.\n",
    "1. Use the [`matminer`](https://hackingmaterials.lbl.gov/matminer/) package to featurize chemical formulas.\n",
    "\n",
    "We will progress through most of this exercise together as a group, and if there's a part that's particularly tricky, please do take time to review it.\n",
    "As always, we're happy to hear from you if you have any questions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb32d824",
   "metadata": {},
   "source": [
    "## Contents\n",
    "\n",
    "These exercises are grouped into the following sections:\n",
    "\n",
    "1. [Overview](#Overview)\n",
    "1. [matminer](#matminer)\n",
    "1. [Model complexity](#Model-complexity)\n",
    "1. [Feature selection](#Feature-selection)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede53479",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "[Back to top](#Contents)\n",
    "\n",
    "From our live discussion, we learned that creating physically-meaningful features will **improve interpretability** of our models as well as their **predictive performance**.\n",
    "This is especially true when we have small and sparse datasets, which tends to be the case for MI applications.\n",
    "With the right choice of _typically a few_ features, we might be able to uncover a strong correlation with the target property of interest and simplify our modeling problem.\n",
    "\n",
    "So far, we've been working with materials datasets and building ML models that _already do this_, so this idea may not be too surprising to you.\n",
    "For example, it is _because_ we know that the atomic number is correlated with atomic weight that we might choose to use atomic number as a feature to predict the atomic weight, and it turned out to give good performance, even with a simple model like linear regression.\n",
    "If someone did not know this information, they could use _many more features_ and still _fail to get a good ML model_.\n",
    "We will now show this is the case.\n",
    "\n",
    "### First import everything because we're getting lazy\n",
    "\n",
    "Although, fwiw, if you are sharing notebooks with others, it's not a bad idea to import everything at the top.\n",
    "That way, if a user does not have a package, they find out right away and can go install it rather than discovering this midway through and having to interrupt their flow. 🌊"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a2186e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 300)\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.model_selection import KFold, cross_val_score, cross_val_predict\n",
    "\n",
    "plt.rcParams.update({'figure.figsize':(8,6),       # Increase figure size\n",
    "                     'font.size':20,               # Increase font size\n",
    "                     'mathtext.fontset':'cm',      # Change math font to Computer Modern\n",
    "                     'mathtext.rm':'serif',        # Documentation recommended follow-up\n",
    "                     'lines.linewidth':5,          # Thicker plot lines\n",
    "                     'lines.markersize':12,        # Larger plot points\n",
    "                     'axes.linewidth':2,           # Thicker axes lines (but not too thick)\n",
    "                     'xtick.major.size':8,         # Make the x-ticks longer (our plot is larger!)\n",
    "                     'xtick.major.width':2,        # Make the x-ticks wider\n",
    "                     'ytick.major.size':8,         # Ditto for y-ticks\n",
    "                     'ytick.major.width':2,        # Ditto for y-ticks\n",
    "                     'xtick.direction':'in', \n",
    "                     'ytick.direction':'in'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff8afb56",
   "metadata": {},
   "source": [
    "### Choosing bad features\n",
    "\n",
    "Now we'll choose a few features that are the column names of our elemental properties DataFrame and we'll store these features into a `features` list.\n",
    "We then load in the DataFrame, making sure to drop any rows that have `NaN` in those columns (`df.dropna(subset=features)`) so that we can actually perform linear regression.\n",
    "Feel free to add more features to the list, just don't add `atomic_number`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9707dd05",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['row', 'electronegativity', 'molar_volume']\n",
    "df = pd.read_csv('../../assets/data/week_1/04/elem_props.csv')\n",
    "df = df.dropna(subset=features)\n",
    "print(f'There are {len(df)} rows of the data remaining.')\n",
    "display(df.head())\n",
    "\n",
    "X = df[features]\n",
    "# X = df[['atomic_number']]    # uncomment this line and see what happens!\n",
    "y = df['atomic_mass']\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "lr = LinearRegression()\n",
    "kfold = KFold(n_splits=5, shuffle=True)\n",
    "scores = cross_val_score(lr, X_scaled, y, scoring='neg_root_mean_squared_error', cv=kfold)\n",
    "print(f'The average CV score is {-np.mean(scores):.3f}.')\n",
    "\n",
    "lr.fit(X_scaled, y)\n",
    "yhat = lr.predict(X_scaled)\n",
    "print(f'The slopes are {lr.coef_} and the intercept term is {lr.intercept_}')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7,7))\n",
    "ax.scatter(y, yhat, s=40)\n",
    "yhat = cross_val_predict(lr, X_scaled, y, cv=kfold)\n",
    "ax.plot(y, y, c='k', zorder=-5)\n",
    "ax.set_xlabel('actual atomic weight')\n",
    "ax.set_ylabel('predicted atomic weight')\n",
    "ax.set_aspect('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8bf444e",
   "metadata": {},
   "source": [
    "**Pause and reflect**: Note how we can use many more features to build our ML model and still create the parity plot the same way as before, since it only depends on the output predictions.\n",
    "Based on the parity plot and coefficients, can you tell how your choice of features are being leveraged for the prediction?\n",
    "\n",
    "\n",
    "What you should observe is that using any combination of the other provided features basically sucks for predicting the atomic weight, and as soon as we switch to atomic number, everything is right with the world. 😇\n",
    "This is encouraging, but we know that in the real world, materials properties won't be _this_ strongly dependent on each other, and then it becomes very unclear which features we should choose. \n",
    "Also, features won't always be given to you in a nice data table like the previous example, so is there a way we can programmatically and automatically generate some physically-meaningful features from a material's chemical formula or crystal structure?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac1073ab",
   "metadata": {},
   "source": [
    "## matminer\n",
    "\n",
    "[Back to top](#Contents)\n",
    "\n",
    "![matminer](../../assets/fig/week_1/04/matminer.png)\n",
    "\n",
    "It turns out that there are now _many_ Python packages that are able to generate features, and because we're proud Berkeley Bears, we will use the [`matminer`](https://hackingmaterials.lbl.gov/matminer/) package developed by scientists at UCB and LBL.\n",
    "This package is actually quite popular as a general purpose [featurization tool](https://hackingmaterials.lbl.gov/matminer/featurizer_summary.html), and it will not only be great for teaching featurizations, but we also hope you will use it in your self-directed research projects.\n",
    "One notable feature of matminer is that it mostly uses pandas DataFrames as inputs and outputs, which is great news for us!\n",
    "\n",
    "As a quick aside, we wanted to wait until now to mention that matminer [also has datasets](https://hackingmaterials.lbl.gov/matminer/dataset_summary.html) and [database retrieval objects](https://hackingmaterials.lbl.gov/matminer/index.html#data-retrieval-easily-puts-complex-online-data-into-dataframes) that are pretty cool to look into if you want to explore more data mining on your own.\n",
    "We won't spend too much time discussing these tools, though we will utilize one below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc5efa7",
   "metadata": {},
   "source": [
    "### Featurizing a chemical formula\n",
    "\n",
    "The first thing we can try is to take a chemical formula and enumerate a set of properties based on the formula.\n",
    "This is appealing because chemical formulas seem like a fairly accessible property of the material (otherwise what did you even make??) and chemistry determines a lot of properties, so we'd like to leverage it.\n",
    "\n",
    "To featurize a chemical formula that is represented as a string, we have to first convert it into a Pymatgen [`Composition`](https://github.com/hackingmaterials/matminer/blob/main/matminer/featurizers/conversions.py) object, which allows us to make sense of the elements present, subscripts, etc.\n",
    "As you can imagine, matminer builds off of the functionality in Pymatgen. 😉\n",
    "To perform the actual conversion, we use the [`StrToComposition`](https://hackingmaterials.lbl.gov/matminer/matminer.featurizers.html#matminer.featurizers.conversions.StrToComposition) class from the [`matminer.featurizers.conversions`](https://hackingmaterials.lbl.gov/matminer/featurizer_summary.html#conversions) submodule, and this is called as follows:\n",
    "\n",
    "```python\n",
    "from matminer.featurizers.conversions import StrToComposition\n",
    "str_comp = StrToComposition(target_col_id='composition')\n",
    "data = str_comp.featurize_dataframe(data, col_id='formula')\n",
    "```\n",
    "\n",
    "where:\n",
    "1. The first line is the package import statement.\n",
    "1. The second line constructs the conversion object, and specifies that the **new column** holding the Composition will be named `'composition'`.\n",
    "1. The third line performs the conversion (using [`featurize_dataframe(df, col_id)`](https://hackingmaterials.lbl.gov/matminer/matminer.featurizers.html#matminer.featurizers.conversions.ConversionFeaturizer.featurize_dataframe) and stores the output in a new DataFrame.\n",
    "The `col_id` parameter says that the name of the **existing column** that holds the chemical formulas is named `'formula'`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44563e5d",
   "metadata": {},
   "source": [
    "### Band gap data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9530fe67",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matminer.datasets import load_dataset\n",
    "bg_data = load_dataset('matbench_expt_gap')\n",
    "bg_data = bg_data[bg_data['gap expt'] > 0].reset_index(drop=True)   # just look at positive band gaps, trims dataset\n",
    "display(bg_data.head())                      # pretty straightforward!\n",
    "\n",
    "from matminer.featurizers.conversions import StrToComposition\n",
    "str_to_comp = StrToComposition(target_col_id='composition_pmg')\n",
    "bg_data_comp = str_to_comp.featurize_dataframe(bg_data, col_id='composition')\n",
    "display(bg_data_comp.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfceb1da",
   "metadata": {},
   "source": [
    "### Enumerating elemental properties\n",
    "\n",
    "![Magpie features](../../assets/fig/week_1/04/magpie_features.png)\n",
    "\n",
    "Once we have a column of Composition objects, we're ready to perform the featurization!\n",
    "We will use one of the featurizers from the [`matminer.featurizers.composition`](https://hackingmaterials.lbl.gov/matminer/featurizer_summary.html#composition) submodule, namely [`ElementProperty`](https://hackingmaterials.lbl.gov/matminer/matminer.featurizers.composition.html#matminer.featurizers.composition.composite.ElementProperty) (although there are many, _many_ more).\n",
    "For the `ElementProperty`, we will demonstrate the [`featurizer.from_preset('magpie')`](https://hackingmaterials.lbl.gov/matminer/matminer.featurizers.composition.html#matminer.featurizers.composition.composite.ElementProperty.from_preset) class method, which creates an object that can automatically engineer **132 new features** based on the method by [Ward et al. _npj Comput. Mater._, 2016](https://www.nature.com/articles/npjcompumats201628). 🤯\n",
    "\n",
    "It does this by first calculating what the property (e.g., electronegativity) is for _each element_ in the composition.\n",
    "It then computes a set of **statistical quantities** based on the set of elemental properties for each composition, such as the mean, minimum, maximum, etc.\n",
    "These statistical quantities of the elemental properties then become the new features for that material.\n",
    "\n",
    "The `ElementProperty` object can then call the [`featurize_dataframe(df, col_id='comp_col_name')`](https://hackingmaterials.lbl.gov/matminer/matminer.featurizers.html#matminer.featurizers.base.BaseFeaturizer.featurize_dataframe) method to generate all the columns with features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a1b2dfe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from matminer.featurizers.composition import ElementProperty\n",
    "featurizer = ElementProperty.from_preset('magpie')\n",
    "bg_data_featurized = featurizer.featurize_dataframe(bg_data_comp, col_id='composition_pmg')\n",
    "y = bg_data_featurized['gap expt']\n",
    "bg_data_featurized.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aaa9593",
   "metadata": {},
   "source": [
    "**Pause and reflect**: Do you recognize any of the column headings? Why are we taking the average, min, max, etc. of the elemental features?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ecbcf0",
   "metadata": {},
   "source": [
    "### Remove correlated features\n",
    "\n",
    "So far we've covered several best practices, such as scaling our input features for linear regression and performing cross validation to get a realistic assessment, and here we introduce one more.\n",
    "Whenever we're working with more than one feature, there is a chance that a few of them will be very strongly [linearly] correlated.\n",
    "We've already discussed what the Pearson correlation coefficient ($r$) means, and the reason this is undesirable is because the correlation can influence feature importance and potentially cause numerical instabilities during fitting.\n",
    "\n",
    "#### How can we spot these correlations and remove them from our dataset?\n",
    "\n",
    "You already know the answer to the first part, which is by plotting a heatmap!\n",
    "Given a DataFrame, recall that we can compute the correlation between the columns using the [`df.corr()`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.corr.html) method.\n",
    "Then we can plot a pretty heatmap. 😊"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d765d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "bg_data_featurized = bg_data_featurized.dropna(how='any', axis=1)   # this removes columns w/ NaN\n",
    "corr_matrix = bg_data_featurized.corr().abs()   # we take the absolute value\n",
    "fig, ax = plt.subplots()\n",
    "h = ax.imshow(corr_matrix)\n",
    "plt.colorbar(h)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acefe32d",
   "metadata": {},
   "source": [
    "Cool!\n",
    "But maybe unlike the previous exercises where it was clear what each column and row represented, here there are just too many features involved.\n",
    "And so, this calls for an _automated, programmatic approach_, which is what the following code does.\n",
    "It removes the following columns:\n",
    "- For any two features that have a correlation $|r| > 0.95$, it removes one of them.\n",
    "- Any of the columns with more than 100 `0` values.\n",
    "This means the feature is `0` for > 95% of the data, and while it might not strictly speaking be necessary, we just won't care about them here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1366a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the upper triangular portion of correlation matrix, to avoid double counting/removing\n",
    "upper_tri = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "\n",
    "# figure out which columns to drop\n",
    "to_drop = [column for column in upper_tri.columns if any(upper_tri[column] > 0.95)]\n",
    "to_drop += list(bg_data_featurized.columns[bg_data_featurized.isin([0]).sum() > 100])\n",
    "\n",
    "# drop those columns\n",
    "bg_data_cleaned = bg_data_featurized.drop(to_drop, axis=1)\n",
    "display(bg_data_cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d28a68a1",
   "metadata": {},
   "source": [
    "### Finally, finally, _finally_ build the ML model for CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7292457",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression()\n",
    "kfold = KFold(n_splits=5, shuffle=True)\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# only get the features (not the compositions, and certainly not the bandgap values!)\n",
    "X = bg_data_cleaned.loc[:, featurizer.feature_labels()[0]:]\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "y = bg_data_cleaned['gap expt']\n",
    "\n",
    "# run cross validations AND store the predict values\n",
    "yhat = cross_val_predict(lr, X_scaled, y, cv=kfold)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5,5))\n",
    "ax.scatter(y, yhat, s=40)\n",
    "ax.plot(y, y, c='k', zorder=-5)\n",
    "ax.set_xlabel('actual band gap (eV)')\n",
    "ax.set_ylabel('predicted band gap (eV)')\n",
    "ax.set_xlim(-1, 13)\n",
    "ax.set_ylim(-3, 13)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "420a7dfb",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "    <center><b>~ BREAK ~</b></center>\n",
    "    At this point, we're going to give ourselves a short break before continuing further.\n",
    "    Great work so far! 💪\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be3c4d0b",
   "metadata": {},
   "source": [
    "## Model complexity\n",
    "\n",
    "[Back to top](#Contents)\n",
    "\n",
    "Wow, this featurization strategy is pretty cool, even if the new features can't perfectly predict experimental band gaps.\n",
    "At least now we have an automated way to generate a bunch of physically-meaningful and interpretable features from the chemical formula, and it's conceivable that we can add _even more features_ from matminer or other packages/datasets, particularly if we account for the crystal structure.\n",
    "\n",
    "But, if we continued in this way of blindly generating more and more features for ML, hoping to improve our model performance, then we'd actually fall for a trap! 🕳 \n",
    "Note that each time we add a feature, at least for linear regression, we also add a parameter (another coefficient) to the linear regression model.\n",
    "The addition of another parameter adds another degree of freedom that we must optimize for and increases the **model complexity**.\n",
    "\n",
    "It is true that a more complex model should be able to learn more nuances _in the data that it was trained on_.\n",
    "Hence, a more complex model will generally have lower **training error** than a simpler model (fewer parameters).\n",
    "After all, physicist John von Neumann has [famously said](https://www.nature.com/articles/427297a), \"With four parameters I can fit an elephant, and with five I can make him wiggle his trunk.\" 🐘\n",
    "\n",
    "But what about _unseen data_, like the validation set in $k$-fold CV? \n",
    "If our model is a really complex polynomial and we have noisy or irregular data (always true in materials science), then it might not be a good approximation for new data, even when that data is an _interpolation_ rather than _extrapolation_ (where it would presumably do even worse).\n",
    "Thus at some point, **the validation error will begin to increase** with model complexity, _even as the training error continues to decrease_.\n",
    "At this point we say the model has started to **overfit** to the training data, and before this point, we might say the model is **underfitting**.\n",
    "At the point where validation error bottoms out is the Goldilocks sweet spot. 🐻\n",
    "\n",
    "The code below illustrates this point by performing linear regression with increasingly complex polynomial features and plotting (a) the model on the left, and (b) the training and validation errors on the right.\n",
    "Note that we wrote several helper functions in a separate file to remove some of the clutter.\n",
    "For example, `gen_data()` is not some \"well-known\" function available everywhere; it was hard-coded for this example.\n",
    "Nonetheless, by revealing most of the important details, we hope to prove that we are not \"gaming\" anything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4388b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper_funcs_04 import *\n",
    "\n",
    "# generate some data\n",
    "dom = np.reshape(np.linspace(0, 6, 1000), (-1, 1))\n",
    "X_train, y_train, X_test, y_test = gen_data(dom)\n",
    "\n",
    "# create plot objects and initial points on left side\n",
    "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(17, 6))\n",
    "ax[0].scatter(X_train, y_train, label='training pts')\n",
    "ax[0].scatter(X_test, y_test, label='test pts')\n",
    "\n",
    "# loop through increasingly complex models\n",
    "train_errs, val_errs = [], []\n",
    "degree = 8\n",
    "for i in range(1, degree):\n",
    "\n",
    "    # ML fitting helper function - return model and errors\n",
    "    y_fit, train_err, val_err = ml_fitting(i, dom, X_train, y_train, X_test, y_test)\n",
    "    train_errs.append(train_err)\n",
    "    val_errs.append(val_err)\n",
    "    \n",
    "    # plot the low-degree polynomials for illustration on left side\n",
    "    if i < 5:\n",
    "        ax[0].plot(dom, y_fit, lw=3, label=f'degree {i}', c=f'C{i+1}')\n",
    "\n",
    "# plots errors on the right and some nice styling\n",
    "ax[1].plot(np.arange(1, degree), train_errs, label='training error')\n",
    "ax[1].plot(np.arange(1 ,degree), val_errs, label='validation error')\n",
    "plot_styling(ax, degree)\n",
    "# fig.savefig('../../assets/fig/week_1/04/model_complexity.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a71091c",
   "metadata": {},
   "source": [
    "## Feature selection\n",
    "\n",
    "[Back to top](#Contents)\n",
    "\n",
    "While the previous example used a non-materials example, we hope you'll believe us when we say that it happens with materials data too, more often than we would like. 😢\n",
    "Therefore, we need a way to be able to perform **feature selection** after we have finished generating all of our features using matminer.\n",
    "\n",
    "Once again, scikit-learn generously provides many built-in feature selection classes in the [`sklearn.feature_selection`](https://scikit-learn.org/stable/modules/feature_selection.html) module.\n",
    "We're going to go ahead and use a rather simple [univariate feature selection](https://scikit-learn.org/stable/modules/feature_selection.html#univariate-feature-selection) tool called [`SelectKBest(score_func, k)`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectKBest.html) that has the following parameters:\n",
    "\n",
    "- `score_func`: A scoring function callable that, for regression tasks, can either be [`f_regression`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.f_regression.html) or [`mutual_info_regression`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.mutual_info_regression.html).\n",
    "These are functions imported from the `sklearn.feature_selection` module, and as callables we only need to write their name _without parentheses_ to the function arguments.\n",
    "- `k`: An `int` for the number of top features to select.\n",
    "\n",
    "**Pause and reflect**: How do we find the optimal value of `k`?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be500db7",
   "metadata": {},
   "source": [
    "### Exercise: apply feature selection to the band gap prediction problem and identify the top 10 features.\n",
    "\n",
    "If you didn't change anything, the featurized data should be stored in `X`.\n",
    "\n",
    "**Challenge**: Can you then use only those 10 features to build a linear regression model?\n",
    "Does the model perform better or worse?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992e3e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, f_regression, mutual_info_regression\n",
    "selector = SelectKBest(score_func=f_regression, k=10)\n",
    "selector_fitted = selector.fit(X, y)\n",
    "selected_indices = selector_fitted.get_support(indices=False)\n",
    "print(selected_indices)\n",
    "# -------------   WRITE YOUR CODE IN THE SPACE BELOW   ---------- #\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e529c9",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Congratulations on making it to the end! 🎉👏\n",
    "\n",
    "There is a lot of information that we covered in this notebook, so it's totally reasonable if not everything clicked right away and you find yourself needing to revist these concepts.\n",
    "\n",
    "Please don't hesitate to reach out on Slack if you have questions or concerns."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
