{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5334554e",
   "metadata": {},
   "source": [
    "# Introduction to ML notebook\n",
    "\n",
    "*Authors: Enze Chen and Mark Asta (University of California, Berkeley)*\n",
    "\n",
    "```{note}\n",
    "This is an interactive exercise, so you will want to click the {fa}`rocket` and open the notebook in DataHub (or Colab for non-UCB students).\n",
    "```\n",
    "\n",
    "## Learning objectives\n",
    "\n",
    "This notebook contains a series of exercises that will teach you the basics of the [scikit-learn package](https://scikit-learn.org/stable/), which is a very popular framework for ML in Python.\n",
    "We will start with a discussion of common terms and best practices so that everyone is on the same page.\n",
    "By the end of this lesson, you will be able to:\n",
    "1. Define the common terminology used in ML.\n",
    "1. Write a vanilla gradient descent algorithm for linear regression.\n",
    "1. Use scikit-learn to construct simple regression and classification models.\n",
    "\n",
    "We will progress through most of this exercise together as a group and are happy to take questions you might have.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb32d824",
   "metadata": {},
   "source": [
    "## Contents\n",
    "\n",
    "These exercises are grouped into the following sections:\n",
    "\n",
    "1. [Overview of ML](#Overview-of-ML)\n",
    "1. [Regression by hand](#Regression-by-hand)\n",
    "1. [Regression with scikit-learn](#Regression-with-scikit-learn)\n",
    "1. [Classification](#Classification)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f921d7",
   "metadata": {},
   "source": [
    "## Overview of ML\n",
    "\n",
    "[Back to top](#Contents)\n",
    "\n",
    "We will start by covering some fundamental ideas and terminology in ML.\n",
    "These were all discussed in the live Zoom session, so we will only provide a _quick_ summary here.\n",
    "\n",
    "The three broad types of ML problems/applications are \n",
    "1. **Regression** for predicting a numerical value.\n",
    "1. **Classification** for predicting a categorical value.\n",
    "1.  _Clustering_ for identifying structure in data.\n",
    "\n",
    "A roughly parallel classification in terms of learning algorithms is\n",
    "1. **Supervised** learning, where inputs and outputs are given.\n",
    "Regression and classification mostly correspond to supervised learning.\n",
    "1. _Unsupervised_ learning, where inputs are given but outputs are not.\n",
    "Clustering mostly corresponds to unsupervised learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee5cbb1",
   "metadata": {},
   "source": [
    "## Regression by hand\n",
    "\n",
    "[Back to top](#Contents)\n",
    "\n",
    "We will start with supervised learning algorithms for regression, where the input data in **design matrix** $X \\in \\mathbb{R}^{m \\times n}$ and output **target vector** $\\vec{y} \\in \\mathbb{R}^{m}$ are both provided.\n",
    "\n",
    "Each row of $X$ and $\\vec{y}$ is a corresponding **example** or data point (so there are $m$ examples), and each column of $X$ is a **feature** (so there are $n$ features).\n",
    "\n",
    "Our job is to learn a function/model $h(X, \\vec{\\theta})$ that tries to map $X \\rightarrow \\vec{y}$ using a set of **parameters** $\\vec{\\theta} \\in \\mathbb{R}^{n}$.\n",
    "So far this is a general formulation.\n",
    "\n",
    "The model $h(X, \\vec{\\theta})$ can vary in complexity, and generally we classify the behavior as **linear** and nonlinear, where linear here specifically refers to \"a function that is linear _in the parameters_ $\\vec{\\theta}$.\"\n",
    "This means the function will have linear terms $\\theta_1$, $\\theta_2$, etc., but no cross-terms like $\\theta_1\\theta_2$.\n",
    "We can imagine one such linear model could be:\n",
    "\n",
    "$$ h(X, \\vec{\\theta}) = X \\vec{\\theta} = \\theta_1 \\vec{x}_1 + \\cdots + \\theta_n \\vec{x}_n = \\hat{y} $$\n",
    "\n",
    "which is the vectorized form of multivariable **linear regression**.\n",
    "Our goal is to find a set of parameters $\\vec{\\theta}$ such that $X \\vec{\\theta} = \\hat{y}$ closely approximates $\\vec{y}$.\n",
    "$\\hat{y}$ are the model's **predictions** and the process of learning the parameters $\\vec{\\theta}$ is called **training** the model.\n",
    "\n",
    "---\n",
    "\n",
    "**Pause and reflect**: Why might we start with a linear model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b3e1c9b",
   "metadata": {},
   "source": [
    "### Cost function\n",
    "\n",
    "How can we measure if we're close?\n",
    "For a single example $\\vec{x}^{(i) \\top} \\in \\mathbb{R}^{n}$ in row $i$ of $X$, we can compute the **squared error**:\n",
    "\n",
    "$$ (\\vec{x}^{(i) \\top} \\vec{\\theta} - y^{(i)})^2 $$\n",
    "\n",
    "If we want to find the total error across all examples, we then add across all rows $i$:\n",
    "\n",
    "$$ \\sum_{i=1}^{m} (\\vec{x}^{(i) \\top} \\vec{\\theta} - y^{(i)})^2 \\tag{1} $$\n",
    "\n",
    "If we think a little bit about this expression and try to express the same idea in matrix-vector notation, we can rewrite Eq 1 as:\n",
    "\n",
    "$$ || X \\vec{\\theta} - \\vec{y} ||_2^2 = || \\hat{y} - \\vec{y} ||_2^2 $$\n",
    "\n",
    "Another words to describe this quantity is the **cost function**, symbolized $J$, which measures the \"price we pay\" (penalty) for this approximate linear model.\n",
    "It is customary to include a factor of $\\frac{1}{2}$ in the cost function for reasons you'll see shortly, and _normalize_ the cost function by the number of examples $m$.\n",
    "Thus, the cost function for our linear regression model is:\n",
    "\n",
    "$$ J(\\vec{\\theta}) = \\frac{1}{2m} || X \\vec{\\theta} - \\vec{y} ||_2^2 \\tag{2} $$\n",
    "\n",
    "----\n",
    "\n",
    "**Pause and reflect**: Why is $J$ a function of $\\vec{\\theta}$?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e293e1",
   "metadata": {},
   "source": [
    "### Learning through optimization\n",
    "\n",
    "Now that we have an expression for the penalty (Eq 2), we have a path forward towards learning the \"best model\" by trying to minimize this function.\n",
    "And we know that to minimize functions, we have to take a derivative, which in higher dimensions you'll know as the **gradient**, and set the result equal to $0$.\n",
    "\n",
    "In multivariable calculus, you took the gradient of a scalar potential (like gravitation, electric, or arbitrary), but here we're taking the gradient of _a vector norm_ and applying chain rule to _a matrix-vector product_.\n",
    "While the details are unfortunately outside the scope of this lesson, the result is:\n",
    "\n",
    "$$ \\nabla_{\\theta} J(\\vec{\\theta}) = \\frac{1}{m} X^{\\top} (X \\vec{\\theta} - \\vec{y}) \\tag{3} $$\n",
    "\n",
    "We can solve for the optimal parameters $\\vec{\\theta}$ in two ways.\n",
    "The first, as alluded to above, is to set the gradient equal to $\\vec{0}$ since that will be the location of the minimum.\n",
    "Doing this and rearranging terms, we get the **normal equation** solution to least-squares linear regression:\n",
    "\n",
    "$$ \\vec{\\theta} = (X^{\\top}X)^{-1} X^{\\top} \\vec{y} \\tag{4} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad1a904",
   "metadata": {},
   "source": [
    "### Gradient descent\n",
    "\n",
    "The second is through gradient descent, which is an iterative approach.\n",
    "_In parameter space_, we are searching for the minimum of $J(\\vec{\\theta})$ which is a **convex function**.\n",
    "This means we can make an initial guess for $\\vec{\\theta}$ and slowly move in the direction opposite the gradient at that point to get ourselves closer to the minimum.\n",
    "This is the idea behind **gradient descent** (or _ascent_ to find the maximum) and the update rule looks like:\n",
    "\n",
    "$$ \\vec{\\theta}_{\\text{new}} := \\vec{\\theta}_{\\text{old}} - \\alpha \\nabla_{\\theta} J(\\vec{\\theta_{\\text{old}}}) \\tag{5} $$\n",
    "\n",
    "where $\\alpha$ is a step size or **learning rate**.\n",
    "By repeating this update rule again and again (and again...) we can arrive at our solution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb664261",
   "metadata": {},
   "source": [
    "### Exercise: implement the normal equation and gradient descent for linear regression\n",
    "\n",
    "We will return to the problem from the first day of predicting the atomic weight of an element from the atomic number.\n",
    "Recall that the physical relationship is:\n",
    "\n",
    "$$ \\text{atomic weight = atomic number + weighted-average number of neutrons} $$\n",
    "\n",
    "#### Import Python packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d378adef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.rcParams.update({'figure.figsize':(8,6),       # Increase figure size\n",
    "                     'font.size':20,               # Increase font size\n",
    "                     'mathtext.fontset':'cm',      # Change math font to Computer Modern\n",
    "                     'mathtext.rm':'serif',        # Documentation recommended follow-up\n",
    "                     'lines.linewidth':5,          # Thicker plot lines\n",
    "                     'lines.markersize':12,        # Larger plot points\n",
    "                     'axes.linewidth':2,           # Thicker axes lines (but not too thick)\n",
    "                     'xtick.major.size':8,         # Make the x-ticks longer (our plot is larger!)\n",
    "                     'xtick.major.width':2,        # Make the x-ticks wider\n",
    "                     'ytick.major.size':8,         # Ditto for y-ticks\n",
    "                     'ytick.major.width':2,        # Ditto for y-ticks\n",
    "                     'xtick.direction':'in', \n",
    "                     'ytick.direction':'in'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d77e8ca",
   "metadata": {},
   "source": [
    "### Normal equations - as a group!\n",
    "\n",
    "We'll import the data from the `number_weight.npy` file, which is structured as\n",
    "\n",
    "$$ \\text{data}\\ = \\begin{bmatrix} 1 & 1.008 \\\\ 2 & 4.003 \\\\ \\vdots & \\vdots \\\\ 49 & 114.818 \\\\ 50 & 118.71 \\end{bmatrix} $$\n",
    "\n",
    "and what we want is\n",
    "\n",
    "$$ X \\equiv \\begin{bmatrix} \\vec{1} & \\vec{Z} \\end{bmatrix} = \\begin{bmatrix} 1 & 1 \\\\ 1 & 2 \\\\ \\vdots & \\vdots \\\\ 1 & 49 \\\\ 1 & 50 \\end{bmatrix}, \\quad\n",
    "\\vec{y} = \\begin{bmatrix} 1.008 \\\\ 4.003 \\\\ \\vdots \\\\ 114.818 \\\\ 118.71 \\end{bmatrix} $$\n",
    "\n",
    "_Hints_:\n",
    "- [`np.column_stack()`](https://numpy.org/doc/stable/reference/generated/numpy.column_stack.html) arranges 1D arrays vertically into a 2D array.\n",
    "- [`np.reshape(arr, (-1, 1))`](https://numpy.org/doc/stable/reference/generated/numpy.reshape.html) can take a 1D array and force it into a column vector.\n",
    "- `@` is the matrix multiplication operator, and `X.T` is the transpose of array `X`.\n",
    "- [`np.linalg.inv(arr)`](https://numpy.org/doc/stable/reference/generated/numpy.linalg.inv.html) computes the inverse of a 2D array.\n",
    "- We can plot the result now with our matplotlib prowess.\n",
    "Let's see what atomic weights our model predicts for the original atomic numbers.\n",
    "This process of evaluating our trained model on data to get predictions $\\hat{y}$ is called **testing**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f00ac22",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load('../../assets/data/week_1/01/number_weight.npy')\n",
    "# -------------   WRITE YOUR CODE IN THE SPACE BELOW   ---------- #\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "790c2c56",
   "metadata": {},
   "source": [
    "### Gradient descent - as a group!\n",
    "\n",
    "Recall that the cost function is given by:\n",
    "\n",
    "$$ J(\\vec{\\theta}) = \\frac{1}{2m} || X \\vec{\\theta} - \\vec{y} ||_2^2 \\tag{2} $$\n",
    "\n",
    "and its gradient wrt $\\vec{\\theta}$ is\n",
    "\n",
    "$$ \\nabla_{\\theta} J(\\vec{\\theta}) = \\frac{1}{m} X^{\\top} (X \\vec{\\theta} - \\vec{y}) \\tag{3} $$\n",
    "\n",
    "The gradient descent update rule is:\n",
    "\n",
    "$$ \\vec{\\theta}_{\\text{new}} := \\vec{\\theta}_{\\text{old}} - \\alpha \\nabla_{\\theta} J(\\vec{\\theta_{\\text{old}}}) \\tag{5} $$\n",
    "\n",
    "_Hints_:\n",
    "- Write separate helper functions for the cost function (Eq 2) and the gradient update step (Eq 5).\n",
    "- Initialize $\\vec{\\theta} = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$ and $\\alpha = 10^{-5}$.\n",
    "- How do we know when to stop?\n",
    "- We can plot this result too!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb67e852",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------   WRITE YOUR CODE IN THE SPACE BELOW   ---------- #\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c94c0252",
   "metadata": {},
   "source": [
    "### Animated version\n",
    "\n",
    "To showcase another cool matplotlib module, [`matplotlib.animation`](https://matplotlib.org/stable/api/animation_api.html), we will _animate_ the process of gradient descent using the following code so you can visualize the algorithm learning the parameters with each step.\n",
    "Seeing is believing, as they say. 👀\n",
    "Note that we're stopping it early so it's not yet fully optimized, but the remaining iterations aren't too interesting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1aae237",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.animation import FuncAnimation\n",
    "from matplotlib.artist import Artist\n",
    "data = np.load('../../assets/data/week_1/01/number_weight.npy')\n",
    "m = len(data)\n",
    "X = np.column_stack([np.ones(m), data[:, 0]])\n",
    "y = np.reshape(data[:, 1], (-1, 1))\n",
    "theta = np.zeros((2, 1))\n",
    "alpha = 3e-5\n",
    "cost = 1 / (2 * m) * np.linalg.norm(X @ theta - y) ** 2\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(data[:, 0], data[:, 1], label='data')\n",
    "h, = ax.plot([], [], c='gray', label='regression')\n",
    "ax.set_xlabel('atomic number')\n",
    "ax.set_ylabel('atomic weight')\n",
    "ax.legend(loc=(0.55, 0.2), fontsize=18)\n",
    "t = ax.text(0, 0, '')\n",
    "u = ax.text(0, 0, '')\n",
    "v = ax.text(0, 0, '')\n",
    "plt.tight_layout()\n",
    "plt.close()\n",
    "\n",
    "def init():\n",
    "    return h,\n",
    "    \n",
    "def animate(i):\n",
    "    global cost, theta, X, y, ax, t, u, v\n",
    "    h.set_data(data[:, 0], X @ theta)\n",
    "    Artist.remove(t)\n",
    "    Artist.remove(u)\n",
    "    Artist.remove(v)\n",
    "    box = dict(fc='1.0', ec='white')\n",
    "    t = ax.text(2, 110, s=f'iteration {i}', fontsize=18, ha='left', bbox=box)\n",
    "    u = ax.text(2, 95, s=f'cost = {cost:.3f}', fontsize=18, ha='left', bbox=box)\n",
    "    v = ax.text(2, 80, s=rf\"$\\theta$ = {theta.ravel()}\", fontsize=18, ha='left', bbox=box)\n",
    "    \n",
    "    theta = theta - alpha * (X.T @ (X @ theta - y))\n",
    "    cost = 1 / (2 * m) * np.linalg.norm(X @ theta - y) ** 2\n",
    "    return h, t, u, v\n",
    "\n",
    "plt.rcParams.update({'animation.html': 'jshtml'})\n",
    "np.set_printoptions(precision=3)\n",
    "anim = FuncAnimation(fig, animate, init_func=init, frames=30, interval=300, repeat=False);\n",
    "anim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6168dfa3",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "    <center><b>~ BREAK ~</b></center>\n",
    "    At this point, we're going to give ourselves a short break before continuing further.\n",
    "    Great work so far! 💪\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5593c78",
   "metadata": {},
   "source": [
    "## Regression with scikit-learn\n",
    "\n",
    "[Back to top](#Contents)\n",
    "\n",
    "Now that you've cleared the rite of passage in ML, it's time to introduce a package that will simplify our lives. 🙏\n",
    "[Scikit-learn](https://scikit-learn.org/stable/) is one of the most popular packages for ML in Python, and it's designed in a modular way to be extremely user-friendly.\n",
    "It has all sorts of ML algorithms implemented for us and has great documentation with examples.\n",
    "Perhaps one of the annoying downsides is that the package is so modular that some of the sub-modules can be pretty hard to find sometimes, but luckily you are all expert documentation searchers. 🕵️‍♀️🕵️‍\n",
    "\n",
    "To use scikit-learn, the typical import statement is:\n",
    "```python\n",
    "from sklearn.module import ClassWeWant\n",
    "```\n",
    "where you'll note that the root package name is `sklearn`!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d6c41e",
   "metadata": {},
   "source": [
    "**Step 1**: As an example, to perform linear regression, we import the [`LinearRegression`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html) class from the [`linear_model`](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.linear_model) module as follows:\n",
    "```python\n",
    "from sklearn.linear_model import LinearRegression\n",
    "```\n",
    "\n",
    "**Step 2**: Once we create an object from this class (e.g., `lr = LinearRegression()`), we can train the model on a design matrix $X$ and target vector $y$ using the [`lr.fit(X, y)`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html#sklearn.linear_model.LinearRegression.fit) method, which operates **in place**.\n",
    "Note that if $X$ has a column of $\\vec{1}$, then we will also want to pass `fit_intercept=False` into this method.\n",
    "If our design matrix does not have an intercept term explicitly encoded, then we can leave it as the default (`True` for fit intercept term separately).\n",
    "\n",
    "**Step 3**: A trained model has coefficients that are accessible through the `lr.coef_` attribute (`ndarray` type).\n",
    "If the intercept term was fit separately (not in design matrix), then that is accessed through the `intercept_` attribute (also `ndarray` type).\n",
    "We can use these coefficients to create the equation for a line of best fit.\n",
    "\n",
    "We can also use a trained model to make predictions on new data $X' \\in \\mathbb{R}^{p \\times n}$ by calling [`lr.predict(Xp)`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html#sklearn.linear_model.LinearRegression.predict), which returns a vector of predictions $\\hat{y}$.\n",
    "In other cases, we'd like to plot the predicted values—more on this shortly!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac6fa25",
   "metadata": {},
   "source": [
    "### Exercise: use scikit-learn to train a linear regression model for the atomic weights problem and plot the line of best fit\n",
    "\n",
    "_Hint_:\n",
    "- We've loaded the **training data** for you, and the process to fit the data should take only 3 lines of code(!!)\n",
    "- Once you've fit the model, we suggest you print out the coefficients and intercept terms. Do they match the results from above? 😀"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab1b4af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load('../../assets/data/week_1/01/number_weight.npy')\n",
    "X = np.reshape(data[:, 0], (-1, 1))\n",
    "y = np.reshape(data[:, 1], (-1, 1))\n",
    "# -------------   WRITE YOUR CODE IN THE SPACE BELOW   ---------- #\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb81536",
   "metadata": {},
   "source": [
    "## Classification\n",
    "\n",
    "[Back to top](#Contents)\n",
    "\n",
    "Now let's try our hand at a _classification problem_, where our goal is to build a model that can learn the correct **labels** for our data, such as `{metal, non-metal}`.\n",
    "These labels are also stored in our target vector $\\vec{y}$, except now the values in $\\vec{y}$ can only take on a few discrete values instead of any real number.\n",
    "To simplify the problem even further, we will consider the problem of **binary classification**, where there are only two **classes** (labels) for us to consider.\n",
    "One way to programmatically represent these classes is with the values $\\{0, 1\\}$, which conveniently makes the **logistic function** a good model for classifying this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b4a93b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "x = np.linspace(-6, 6, 100)\n",
    "y = 1 / (1 + np.exp(-x))\n",
    "b = np.zeros(len(x))\n",
    "t = np.ones(len(x))\n",
    "ax.plot(x, b, 'gray', ls='dashed')\n",
    "ax.plot(x, t, 'gray', ls='dashed')\n",
    "ax.plot(x, y)\n",
    "ax.text(-5, 0.7, s=r'$g(z) = \\dfrac{1}{1 + e^{-z}}$')\n",
    "ax.set_xlabel('$z$')\n",
    "ax.set_ylabel('$g(z)$', rotation=0, labelpad=35)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c815b88",
   "metadata": {},
   "source": [
    "As can be seen from the plot, the logistic function is bounded between $g(z) \\in (0, 1)$.\n",
    "For very large (positive) values of $z$, the function approaches $1$, while for very small (negative) values of $z$, the function approaches $0$.\n",
    "When the argument $z=0$, the value of the function is $g(z) = 0.5$.\n",
    "\n",
    "This means we could use the output of this function as a **classifier** based on the input argument.\n",
    "When the logistic function has the particular input:\n",
    "\n",
    "$$ h(X, \\vec{\\theta}) = g(X \\vec{\\theta}) = \\dfrac{1}{1 + \\exp(-X \\vec{\\theta})} $$\n",
    "\n",
    "we call this model **logistic regression**.\n",
    "It has a nice **probabilistic interpretation**, where $h(\\cdot)$ is the probability of the data point being class 1 and $1 - h(\\cdot)$ the probability of the data point being class 0.\n",
    "\n",
    "Note that there is \"regression\" in the name of this method, but it's fundamentally for classification!\n",
    "Like linear regression, logistic regression is also a _linear algorithm_ (still only a dot product).\n",
    "Although we will not show it, logistic regression even has the same gradient descent update rule for learning $\\vec{\\theta}$!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6cc5462",
   "metadata": {},
   "source": [
    "### Exercise: use scikit-learn to classify elements by metal, non-metal\n",
    "\n",
    "_Hints_:\n",
    "- You can pick your features!\n",
    "- [`pd.concat([df1, df2, ...], axis=1)`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.concat.html) might be helpful!\n",
    "- After training your model, test it on the original training data.\n",
    "- Please store your predictions in the `yhat` variable and uncomment the rest of the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2dfb121",
   "metadata": {
    "tags": [
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('../../assets/data/week_1/04/elem_props.csv')\n",
    "display(df.head())\n",
    "# -------------   WRITE YOUR CODE IN THE SPACE BELOW   ---------- #\n",
    "\n",
    "# --------------------------------------------------------------- #\n",
    "from pymatgen.util.plotting import periodic_table_heatmap\n",
    "plot_data = {}\n",
    "for i in range(len(yhat)):\n",
    "    plot_data[df['symbol'][i]] = yhat[i]\n",
    "ptable = periodic_table_heatmap(elemental_data=plot_data, cmap='coolwarm')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e529c9",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This concludes the introduction to ML, which admittedly was a lot!\n",
    "We discussed regression and classification problems, which are both solved with supervised learning algorithms.\n",
    "We coded up linear regression with gradient descent by hand to get a feel for the algorithms, but then quickly turned to the scikit-learn package, which we'll continue to rely on for the rest of this module.\n",
    "\n",
    "We'd love to take any questions you may have, and if you need some time to let the information sink in (and questions to form), that's understandable.\n",
    "Next up we're going to discuss various ways of measuring **model performance** to better quantify what makes a \"good\" ML model."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
