{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5334554e",
   "metadata": {},
   "source": [
    "# Introduction to ML notebook\n",
    "\n",
    "*Authors: Enze Chen and Mark Asta (University of California, Berkeley)*\n",
    "\n",
    "```{note}\n",
    "This is an interactive exercise, so you will want to click the {fa}`rocket` and open the notebook in DataHub (or Colab for non-UCB students).\n",
    "```\n",
    "\n",
    "## Learning objectives\n",
    "\n",
    "This notebook contains a series of exercises that will teach you the basics of the [scikit-learn package](https://scikit-learn.org/stable/), which is a very popular framework for ML in Python.\n",
    "We will start with a discussion of common terms and best practices so that everyone is on the same page.\n",
    "By the end of this lesson, you will be able to:\n",
    "1. Define the common terminology used in ML.\n",
    "1. Write a vanilla gradient descent algorithm for linear regression.\n",
    "1. Use scikit-learn to construct simple regression and classification models.\n",
    "\n",
    "We will progress through most of this exercise together as a group and are happy to take questions you might have.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb32d824",
   "metadata": {},
   "source": [
    "## Contents\n",
    "\n",
    "These exercises are grouped into the following sections:\n",
    "\n",
    "1. [Overview of ML](#Overview-of-ML)\n",
    "1. [Regression by hand](#Regression-by-hand)\n",
    "1. [Regression with scikit-learn](#Regression-with-scikit-learn)\n",
    "1. [Classification](#Classification)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8658cc1",
   "metadata": {},
   "source": [
    "## Overview of ML\n",
    "\n",
    "[Back to top](#Contents)\n",
    "\n",
    "We will start by covering some fundamental ideas and terminology in ML.\n",
    "These were all discussed in the live Zoom session, so we will only provide a _quick_ summary here.\n",
    "\n",
    "The three broad types of ML problems/applications are \n",
    "1. **Regression** for predicting a numerical value.\n",
    "1. **Classification** for predicting a categorical value.\n",
    "1.  _Clustering_ for identifying structure in data.\n",
    "\n",
    "A roughly parallel classification in terms of learning algorithms is\n",
    "1. **Supervised** learning, where inputs and outputs are given.\n",
    "Regression and classification mostly correspond to supervised learning.\n",
    "1. _Unsupervised_ learning, where inputs are given but outputs are not.\n",
    "Clustering mostly corresponds to unsupervised learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14949512",
   "metadata": {},
   "source": [
    "## Regression by hand\n",
    "\n",
    "[Back to top](#Contents)\n",
    "\n",
    "We will start with supervised learning algorithms for regression, where the input data in **design matrix** $X \\in \\mathbb{R}^{m \\times n}$ and output **target vector** $\\vec{y} \\in \\mathbb{R}^{m}$ are both provided.\n",
    "\n",
    "Each row of $X$ and $\\vec{y}$ is a corresponding **example** or data point (so there are $m$ examples), and each column of $X$ is a **feature** (so there are $n$ features).\n",
    "\n",
    "Our job is to learn a function/model $h(X, \\vec{\\theta})$ that tries to map $X \\rightarrow \\vec{y}$ using a set of **parameters** $\\vec{\\theta} \\in \\mathbb{R}^{n}$.\n",
    "So far this is a general formulation.\n",
    "\n",
    "The model $h(X, \\vec{\\theta})$ can vary in complexity, and generally we classify the behavior as **linear** and nonlinear, where linear here specifically refers to \"a function that is linear _in the parameters_ $\\vec{\\theta}$.\"\n",
    "This means the function will have linear terms $\\theta_1$, $\\theta_2$, etc., but no cross-terms like $\\theta_1\\theta_2$.\n",
    "We can imagine one such linear model could be:\n",
    "\n",
    "$$ h(X, \\vec{\\theta}) = X \\vec{\\theta} = \\theta_1 \\vec{x}_1 + \\cdots + \\theta_n \\vec{x}_n = \\hat{y} $$\n",
    "\n",
    "which is the vectorized form of multivariable **linear regression**.\n",
    "Our goal is to find a set of parameters $\\vec{\\theta}$ such that $X \\vec{\\theta} = \\hat{y}$ closely approximates $\\vec{y}$.\n",
    "$\\hat{y}$ are the model's **predictions** and the process of learning the parameters $\\vec{\\theta}$ is called **training** the model.\n",
    "\n",
    "---\n",
    "\n",
    "**Pause and reflect**: Why might we start with a linear model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb646272",
   "metadata": {},
   "source": [
    "### Cost function\n",
    "\n",
    "How can we measure if we're close?\n",
    "For a single example $\\vec{x}^{(i) \\top} \\in \\mathbb{R}^{n}$ in row $i$ of $X$, we can compute the **squared error**:\n",
    "\n",
    "$$ (\\vec{x}^{(i) \\top} \\vec{\\theta} - y^{(i)})^2 $$\n",
    "\n",
    "If we want to find the total error across all examples, we then add across all rows $i$:\n",
    "\n",
    "$$ \\sum_{i=1}^{m} (\\vec{x}^{(i) \\top} \\vec{\\theta} - y^{(i)})^2 \\tag{1} $$\n",
    "\n",
    "If we think a little bit about this expression and try to express the same idea in matrix-vector notation, we can rewrite Eq 1 as:\n",
    "\n",
    "$$ || X \\vec{\\theta} - \\vec{y} ||_2^2 = || \\hat{y} - \\vec{y} ||_2^2 $$\n",
    "\n",
    "Another words to describe this quantity is the **cost function**, symbolized $J$, which measures the \"price we pay\" (penalty) for this approximate linear model.\n",
    "It is customary to include a factor of $\\frac{1}{2}$ in the cost function for reasons you'll see shortly, and _normalize_ the cost function by the number of examples $m$.\n",
    "Thus, the cost function for our linear regression model is:\n",
    "\n",
    "$$ J(\\vec{\\theta}) = \\frac{1}{2m} || X \\vec{\\theta} - \\vec{y} ||_2^2 \\tag{2} $$\n",
    "\n",
    "----\n",
    "\n",
    "**Pause and reflect**: Why is $J$ a function of $\\vec{\\theta}$?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb12baa7",
   "metadata": {},
   "source": [
    "### Learning through optimization\n",
    "\n",
    "Now that we have an expression for the penalty (Eq 2), we have a path forward towards learning the \"best model\" by trying to minimize this function.\n",
    "And we know that to minimize functions, we have to take a derivative, which in higher dimensions you'll know as the **gradient**, and set the result equal to $0$.\n",
    "\n",
    "In multivariable calculus, you took the gradient of a scalar potential (like gravitation, electric, or arbitrary), but here we're taking the gradient of _a vector norm_ and applying chain rule to _a matrix-vector product_.\n",
    "While the details are unfortunately outside the scope of this lesson, the result is:\n",
    "\n",
    "$$ \\nabla_{\\theta} J(\\vec{\\theta}) = \\frac{1}{m} X^{\\top} (X \\vec{\\theta} - \\vec{y}) \\tag{3} $$\n",
    "\n",
    "We can solve for the optimal parameters $\\vec{\\theta}$ in two ways.\n",
    "The first, as alluded to above, is to set the gradient equal to $\\vec{0}$ since that will be the location of the minimum.\n",
    "Doing this and rearranging terms, we get the **normal equation** solution to least-squares linear regression:\n",
    "\n",
    "$$ \\vec{\\theta} = (X^{\\top}X)^{-1} X^{\\top} \\vec{y} \\tag{4} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bebf12bf",
   "metadata": {},
   "source": [
    "### Gradient descent\n",
    "\n",
    "The second is through gradient descent, which is an iterative approach.\n",
    "_In parameter space_, we are searching for the minimum of $J(\\vec{\\theta})$ which is a **convex function**.\n",
    "This means we can make an initial guess for $\\vec{\\theta}$ and slowly move in the direction opposite the gradient at that point to get ourselves closer to the minimum.\n",
    "This is the idea behind **gradient descent** (or _ascent_ to find the maximum) and the update rule looks like:\n",
    "\n",
    "$$ \\vec{\\theta}_{\\text{new}} := \\vec{\\theta}_{\\text{old}} - \\alpha \\nabla_{\\theta} J(\\vec{\\theta_{\\text{old}}}) \\tag{5} $$\n",
    "\n",
    "where $\\alpha$ is a step size or **learning rate**.\n",
    "By repeating this update rule again and again (and again...) we can arrive at our solution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5714dfec",
   "metadata": {},
   "source": [
    "### Exercise: implement the normal equation and gradient descent for linear regression\n",
    "\n",
    "We will return to the problem from the first day of predicting the atomic weight of an element from the atomic number.\n",
    "Recall that the physical relationship is:\n",
    "\n",
    "$$ \\text{atomic weight = atomic number + weighted-average number of neutrons} $$\n",
    "\n",
    "#### Import Python packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee07e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.rcParams.update({'figure.figsize':(8,6),       # Increase figure size\n",
    "                     'font.size':24,               # Increase font size\n",
    "                     'mathtext.fontset':'cm',      # Change math font to Computer Modern\n",
    "                     'mathtext.rm':'serif',        # Documentation recommended follow-up\n",
    "                     'lines.linewidth':5,          # Thicker plot lines\n",
    "                     'lines.markersize':12,        # Larger plot points\n",
    "                     'axes.linewidth':2,           # Thicker axes lines (but not too thick)\n",
    "                     'xtick.major.size':8,         # Make the x-ticks longer (our plot is larger!)\n",
    "                     'xtick.major.width':2,        # Make the x-ticks wider\n",
    "                     'ytick.major.size':8,         # Ditto for y-ticks\n",
    "                     'ytick.major.width':2,        # Ditto for y-ticks\n",
    "                     'xtick.direction':'in', \n",
    "                     'ytick.direction':'in'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a526df5b",
   "metadata": {},
   "source": [
    "### Normal equations - as a group!\n",
    "\n",
    "We'll import the data from the `number_weight.npy` file, which is structured as\n",
    "\n",
    "$$ \\text{data}\\ = \\begin{bmatrix} 1 & 1.008 \\\\ 2 & 4.003 \\\\ \\vdots & \\vdots \\\\ 49 & 114.818 \\\\ 50 & 118.71 \\end{bmatrix} $$\n",
    "\n",
    "and what we want is\n",
    "\n",
    "$$ X \\equiv \\begin{bmatrix} \\vec{1} & \\vec{Z} \\end{bmatrix} = \\begin{bmatrix} 1 & 1 \\\\ 1 & 2 \\\\ \\vdots & \\vdots \\\\ 1 & 49 \\\\ 1 & 50 \\end{bmatrix}, \\quad\n",
    "\\vec{y} = \\begin{bmatrix} 1.008 \\\\ 4.003 \\\\ \\vdots \\\\ 114.818 \\\\ 118.71 \\end{bmatrix} $$\n",
    "\n",
    "_Hints_:\n",
    "- [`np.column_stack()`](https://numpy.org/doc/stable/reference/generated/numpy.column_stack.html) arranges 1D arrays vertically into a 2D array.\n",
    "- [`np.reshape(arr, (-1, 1))`](https://numpy.org/doc/stable/reference/generated/numpy.reshape.html) can take a 1D array and force it into a column vector.\n",
    "- `@` is the matrix multiplication operator, and `X.T` is the transpose of array `X`.\n",
    "- [`np.linalg.inv(arr)`](https://numpy.org/doc/stable/reference/generated/numpy.linalg.inv.html) computes the inverse of a 2D array.\n",
    "- We can plot the result now with our matplotlib prowess.\n",
    "Let's see what atomic weights our model predicts for the original atomic numbers.\n",
    "This process of evaluating our trained model on data to get predictions $\\hat{y}$ is called **testing**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437c216e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load('../../assets/data/week_1/01/number_weight.npy')\n",
    "# -------------   WRITE YOUR CODE IN THE SPACE BELOW   ---------- #\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e768291",
   "metadata": {},
   "source": [
    "### Gradient descent - as a group!\n",
    "\n",
    "Recall that the cost function is given by:\n",
    "\n",
    "$$ J(\\vec{\\theta}) = \\frac{1}{2m} || X \\vec{\\theta} - \\vec{y} ||_2^2 \\tag{2} $$\n",
    "\n",
    "and its gradient wrt $\\vec{\\theta}$ is\n",
    "\n",
    "$$ \\nabla_{\\theta} J(\\vec{\\theta}) = \\frac{1}{m} X^{\\top} (X \\vec{\\theta} - \\vec{y}) \\tag{3} $$\n",
    "\n",
    "The gradient descent update rule is:\n",
    "\n",
    "$$ \\vec{\\theta}_{\\text{new}} := \\vec{\\theta}_{\\text{old}} - \\alpha \\nabla_{\\theta} J(\\vec{\\theta_{\\text{old}}}) \\tag{5} $$\n",
    "\n",
    "_Hints_:\n",
    "- Write separate helper functions for the cost function (Eq 2) and the gradient update step (Eq 5).\n",
    "- Initialize $\\vec{\\theta} = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$ and $\\alpha = 10^{-5}$.\n",
    "- How do we know when to stop?\n",
    "- We can plot this result too!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "092901ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------   WRITE YOUR CODE IN THE SPACE BELOW   ---------- #\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab57570",
   "metadata": {},
   "source": [
    "### Animated version\n",
    "\n",
    "To showcase another cool matplotlib module, [`matplotlib.animation`](https://matplotlib.org/stable/api/animation_api.html), we will _animate_ the process of gradient descent using the following code so you can visualize the algorithm learning the parameters with each step.\n",
    "Seeing is believing, as they say. 👀\n",
    "Note that we're stopping it early so it's not yet fully optimized, but the remaining iterations aren't too interesting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b651bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.artist import Artist\n",
    "theta = np.zeros((2, 1))\n",
    "alpha = 3e-5\n",
    "cost = 1 / (2 * m) * np.linalg.norm(X @ theta - y) ** 2\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(data[:, 0], data[:, 1], label='data')\n",
    "h, = ax.plot([], [], c='gray', label='regression')\n",
    "ax.set_xlabel('atomic number')\n",
    "ax.set_ylabel('atomic weight')\n",
    "ax.legend(loc=(0.6, 0.3))\n",
    "t = ax.text(0, 0, '')\n",
    "u = ax.text(0, 0, '')\n",
    "v = ax.text(0, 0, '')\n",
    "plt.tight_layout()\n",
    "plt.close()\n",
    "\n",
    "def init():\n",
    "    return h,\n",
    "    \n",
    "def animate(i):\n",
    "    global cost, theta, X, y, ax, t, u, v\n",
    "    h.set_data(data[:, 0], X @ theta)\n",
    "    Artist.remove(t)\n",
    "    Artist.remove(u)\n",
    "    Artist.remove(v)\n",
    "    box = dict(fc='1.0', ec='white')\n",
    "    t = ax.text(2, 110, s=f'iteration {i}', fontsize=18, ha='left', bbox=box)\n",
    "    u = ax.text(2, 95, s=f'cost = {cost:.3f}', fontsize=18, ha='left', bbox=box)\n",
    "    v = ax.text(2, 80, s=rf\"$\\theta$ = {theta.ravel()}\", fontsize=18, ha='left', bbox=box)\n",
    "    \n",
    "    theta = theta - alpha * (X.T @ (X @ theta - y))\n",
    "    cost = 1 / (2 * m) * np.linalg.norm(X @ theta - y) ** 2\n",
    "    return h, t, u, v\n",
    "\n",
    "plt.rcParams.update({'animation.html': 'jshtml'})\n",
    "np.set_printoptions(precision=3)\n",
    "anim = animation.FuncAnimation(fig, animate, init_func=init, frames=30, interval=300, repeat=False);\n",
    "anim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36021a31",
   "metadata": {},
   "source": [
    "## Regression with scikit-learn\n",
    "\n",
    "[Back to top](#Contents)\n",
    "\n",
    "Now that you've cleared the rite of passage in ML, it's time to introduce a package that will simplify our lives. 🙏\n",
    "[Scikit-learn](https://scikit-learn.org/stable/) is one of the most popular packages for ML in Python, and it's designed in a modular way to be extremely user-friendly.\n",
    "It has all sorts of ML algorithms implemented for us and has great documentation with examples.\n",
    "Perhaps one of the annoying downsides is that the package is so modular that some of the sub-modules can be pretty hard to find sometimes, but luckily you are all expert documentation searchers. 🕵️‍♀️🕵️‍\n",
    "\n",
    "To use scikit-learn, the typical import statement is:\n",
    "```python\n",
    "from sklearn.module import ClassWeWant\n",
    "```\n",
    "where you'll note that the root package name is `sklearn`!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aaed572",
   "metadata": {},
   "source": [
    "As an example, to perform linear regression, we import the [`LinearRegression`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html) class from the [`linear_model`](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.linear_model) module as follows:\n",
    "```python\n",
    "from sklearn.linear_model import LinearRegression\n",
    "```\n",
    "\n",
    "Once we create an object from this class, we can train the model on a design matrix $X$ and target vector $y$ using the [`lr.fit(X, y)`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html#sklearn.linear_model.LinearRegression.fit) method, which operates **in place**.\n",
    "\n",
    "Once the model has been trained, we can use it to predict "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f31bce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "331e6cb6",
   "metadata": {},
   "source": [
    "## Classification\n",
    "\n",
    "[Back to top](#Contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e529c9",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Congratulations on making it to the end! 🎉👏\n",
    "\n",
    "There is a lot of information that we covered in this notebook, so it's totally reasonable if not everything clicked right away and you find yourself needing to revist these concepts.\n",
    "\n",
    "Please don't hesitate to reach out on Slack if you have questions or concerns."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
